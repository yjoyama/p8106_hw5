---
title: "Homework5"
author: "Yuki Joyama"
date: "2024-04-28"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)

library(tidyverse)
library(ggplot2)
library(rsample) 
library(caret)
library(kernlab)
library(ISLR)
library(factoextra)

# setup plot theme
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
```

# 1 Support Vector Machine
```{r dataprep}
# read csv files 
df_auto = read_csv("./auto.csv") 
str(df_auto)

# partition (training:test=70:30)
set.seed(100)
data_split = initial_split(df_auto, prop = .70)
train = training(data_split)
test = testing(data_split)
```

## a
Fit a support vector linear classifier to the training data
```{r}
ctrl <- trainControl(method = "cv")

set.seed(100)

svml.fit <- train(
  mpg_cat ~ cylinders + displacement + horsepower + weight + acceleration + year + origin,
  data = train,
  method = "svmLinear",
  tuneGrid = data.frame(C = exp(seq(-4, 5, len = 50))),
  trControl = ctrl
)

plot(svml.fit, highlight = TRUE, xTrans = log)
svml.fit$finalModel
```

I implemented the cross validation to determine the tuning parameter C. In this case, the best parameter C was `r round(svml.fit$bestTune, 3)`.  
The misclassification error rate using the training data is **0.0912**.

```{r}
# test error
test.pred <- predict(svml.fit, newdata = test, type = "raw")

confusionMatrix(
  data = test.pred,
  reference = as.factor(test$mpg_cat)
)
```

The test error rate is 1 - 0.9237 = **0.0763**  

## b
Now, let's fit a support vector machine with a radial kernel to the training data
```{r}
svmr.grid <- expand.grid(
  C = exp(seq(-4, 5, len = 50)),
  sigma = exp(seq(-10, 2, len = 20))
)

set.seed(100)

svmr.fit <- train(
  mpg_cat ~ cylinders + displacement + horsepower + weight + acceleration + year + origin,
  data = train,
  method = "svmRadialSigma",
  tuneGrid = svmr.grid,
  trControl = ctrl
)

svmr.fit$finalModel

myCol <- rainbow(25)
myPar <- list(
  superpose.symbol = list(col = myCol),
  superpose.line = list(col = myCol)
)

plot(svmr.fit, highlight = TRUE, par.settings = myPar)
```

The best tuning parameters selected by the CV are C = `r round(svmr.fit$bestTune[2], 3)` and sigma = `r round(svmr.fit$bestTune[1], 3)`.  
The training error rate is **0.0876**.


```{r}
# test error
test.pred <- predict(svmr.fit, newdata = test, type = "raw")

confusionMatrix(
  data = test.pred,
  reference = as.factor(test$mpg_cat)
)
```

The test error rate is 1 - 0.9237 = **0.0763**

# 2 Hierarchical Clustering 
```{r}
# data prep
data("USArrests")

df_arrest = USArrests |> 
  janitor::clean_names()

str(df_arrest)
```

## a
Hierarchical clustering with complete linkage and Euclidean distance
```{r}
hc.complete <- hclust(dist(df_arrest), method = "complete")

# dendrogram
fviz_dend(
  hc.complete, k = 3,
  cex = 0.5,
  palette = "jco",
  color_labels_by_k = TRUE,
  rect = TRUE, rect_fill = TRUE, rect_border = "jco",
  labels_track_height = 2
)
```

States are clustered as follows:  

Cluster 1 - Florida, North Carolina, Delaware, Alabama, Louisiana, Alaska, Mississippi, South Carolina, Maryland, Arizona, New Mexico, California, Illinois, New York, Michigan, Nevada  
Cluster 2 - Missouri, Arkansas, Tennessee, Georgia, Colorado, Texas, Rhode Island, Wyoming, Oregon, Oklahoma, Virginia, Washington, Massachusetts, New Jersey  
Cluster 3 - Ohio, Utah, Connecticut, Pennsylvania, Nebraska, Kentucky, Montana, Idaho, Indiana, Kansas, Hawaii, Minnesota, Wisconsin, Iowa, New Hampshire, West Virginia, Maine, South Dakota, North Dakota, Vermont  

## b
I will scale the variables to have standard deviation one, then perform the hierarchical clustering with complete linkage and Euclidean distance. 

```{r}
df_arrest_scaled <- df_arrest |> 
  mutate(
    murder = scale(murder)[, 1],
    assault = scale(assault)[, 1],
    rape = scale(rape)[, 1],
    urban_pop = scale(urban_pop)[, 1]
  )

# check
sd(df_arrest_scaled$murder)
sd(df_arrest_scaled$assault)
sd(df_arrest_scaled$urban_pop)
sd(df_arrest_scaled$rape)

hc.complete <- hclust(dist(df_arrest_scaled), method = "complete")

# dendrogram
fviz_dend(
  hc.complete, k = 3,
  cex = 0.5,
  palette = "jco",
  color_labels_by_k = TRUE,
  rect = TRUE, rect_fill = TRUE, rect_border = "jco",
  labels_track_height = 2
)
```

Cluster 1 - South Dakota, West Virginia, North Dakota, Vermont, Maine, Iowa, New Hampshire, Idaho, Montana, Nebraska, Kentucky, Arkansas, Virginia, Wyoming, Missouri, Oregon, Washington, Delaware, Rhode Island, Massachusetts, New Jersey, Connecticut, Minnesota, Wisconsin, Oklahoma, Indiana, Kansas, Ohio, Pennsylvania, Hawaii, Utah  
Cluster 2 - Colorado, California, Nevada, Florida, Texas, Illinois, New York, Arizona, Michigan, Maryland, New Mexico  
Cluster 3 - Alaska, Alabama, Louisiana, Georgia, Tennessee, North Carolina, Mississippi, South Carolina  

## c
Comparing the dendrograms in a and b, we can see that scaling the variables do change the clustering results. This is because the distance measures used in the clustering algorithm are affected by the scale of the variables.   
I think we should scale the variables in this dataset before computing inter-observation dissimilarities for hierarchical clustering because the units of the variables are different between `assault`, `murder`, `rape` and `urban_pop`. This can potentially generate the biased results and scaling can mitigate it to some extent. 
